{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK2g2I75HlN6xpaZo47dFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshuTechie/Sentiment-Analysis-and-Face-Recognition/blob/main/Sentiment_Analysis_%26_Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ_Q_5QGJ4hf",
        "outputId": "dae5758f-54e5-4b2b-f8ef-e6899315c5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04cda7da6ae1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D_d_qV4KOiKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source : https://github.com/aishwaryakore5696/Face-Recognition-and-Emotion-recognition-for-Sentiment-Analysis/blob/main/FaceRecognitionEmotionDetection.ipynb"
      ],
      "metadata": {
        "id": "LCaewUN8OiNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mtcnn"
      ],
      "metadata": {
        "id": "0WF7xKCkKMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm mtcnn was installed correctly\n",
        "import mtcnn\n",
        "# print version\n",
        "print(mtcnn.__version__)"
      ],
      "metadata": {
        "id": "XmIBI7M2KMkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstrate face detection on 5 Celebrity Faces Dataset\n",
        "\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from matplotlib import pyplot\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "# extract a single face from a given photograph\n",
        "def extract_face(filename, required_size):\n",
        "\t# load image from file\n",
        "\timage = Image.open(filename)\n",
        "\t# convert to RGB, if needed\n",
        "\timage = image.convert('RGB')\n",
        "\t# convert to array\n",
        "\tpixels = asarray(image)\n",
        "\t# create the detector, using default weights\n",
        "\tdetector = MTCNN()\n",
        "\t# detect faces in the image\n",
        "\tresults = detector.detect_faces(pixels)\n",
        "\t# extract the bounding box from the first face\n",
        "\tx1, y1, width, height = results[0]['box']\n",
        "\t# bug fix\n",
        "\tx1, y1 = abs(x1), abs(y1)\n",
        "\tx2, y2 = x1 + width, y1 + height\n",
        "\t# extract the face\n",
        "\tface = pixels[y1:y2, x1:x2]\n",
        "\t# resize pixels to the model size\n",
        "\timage = Image.fromarray(face)\n",
        "\timage = image.resize(required_size)\n",
        "\tface_array = asarray(image)\n",
        "\treturn face_array\n",
        "\n",
        "# specify folder to plot\n",
        "folder = './gdrive/My Drive/facenet/images/5-celebrity-faces-dataset/train/ben_afflek/'\n",
        "i = 1\n",
        "# enumerate files\n",
        "for filename in listdir(folder):\n",
        "\t# path\n",
        "\tpath = folder + filename\n",
        "\t# get face\n",
        "\tface = extract_face(path,(160, 160))\n",
        "\tprint(i, face.shape)\n",
        "\t# plot\n",
        "\tpyplot.subplot(2, 7, i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(face)\n",
        "\ti += 1\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "gi5zL3tSKMnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# face detection for the 5 Celebrity Faces Dataset\n",
        "from os import listdir\n",
        "from os.path import isdir\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot\n",
        "from numpy import savez_compressed\n",
        "from numpy import asarray\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "# extract a single face from a given photograph\n",
        "def extract_face(filename, required_size):\n",
        "\t# load image from file\n",
        "\timage = Image.open(filename)\n",
        "\t# convert to RGB, if needed\n",
        "\timage = image.convert('RGB')\n",
        "\t# convert to array\n",
        "\tpixels = asarray(image)\n",
        "\t# create the detector, using default weights\n",
        "\tdetector = MTCNN()\n",
        "\t# detect faces in the image\n",
        "\tresults = detector.detect_faces(pixels)\n",
        "\t# extract the bounding box from the first face\n",
        "\tx1, y1, width, height = results[0]['box']\n",
        "\t# bug fix\n",
        "\tx1, y1 = abs(x1), abs(y1)\n",
        "\tx2, y2 = x1 + width, y1 + height\n",
        "\t# extract the face\n",
        "\tface = pixels[y1:y2, x1:x2]\n",
        "\t# resize pixels to the model size\n",
        "\timage = Image.fromarray(face)\n",
        "\timage = image.resize(required_size)\n",
        "\tface_array = asarray(image)\n",
        "\treturn face_array\n",
        "\n",
        "# load images and extract faces for all images in a directory\n",
        "def load_faces(directory):\n",
        "\tfaces = list()\n",
        "\t# enumerate files\n",
        "\tfor filename in listdir(directory):\n",
        "\t\t# path\n",
        "\t\tpath = directory + filename\n",
        "\t\t# get face\n",
        "\t\tface = extract_face(path,(160, 160))\n",
        "\t\t# store\n",
        "\t\tfaces.append(face)\n",
        "\treturn faces\n",
        "\n",
        "# load a dataset that contains one subdir for each class that in turn contains images\n",
        "def load_dataset(directory):\n",
        "\tX, y = list(), list()\n",
        "\t# enumerate folders, on per class\n",
        "\tfor subdir in listdir(directory):\n",
        "\t\t# path\n",
        "\t\tpath = directory + subdir + '/'\n",
        "\t\t# skip any files that might be in the dir\n",
        "\t\tif not isdir(path):\n",
        "\t\t\tcontinue\n",
        "\t\t# load all faces in the subdirectory\n",
        "\t\tfaces = load_faces(path)\n",
        "\t\t# create labels\n",
        "\t\tlabels = [subdir for _ in range(len(faces))]\n",
        "\t\t# summarize progress\n",
        "\t\tprint('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
        "\t\t# store\n",
        "\t\tX.extend(faces)\n",
        "\t\ty.extend(labels)\n",
        "\treturn asarray(X), asarray(y)\n",
        "\n",
        "\n",
        "#please replace the path mentioned below with the path of your dataset.\n",
        "# load train dataset\n",
        "trainX, trainy = load_dataset('./gdrive/My Drive/facenet/images/5-celebrity-faces-dataset/train/')\n",
        "print(trainX.shape, trainy.shape)\n",
        "# load test dataset\n",
        "testX, testy = load_dataset('./gdrive/My Drive/facenet/images/5-celebrity-faces-dataset/val/')\n",
        "# save arrays to one file in compressed format\n",
        "savez_compressed('5-celebrity-faces-dataset.npz', trainX, trainy, testX, testy)"
      ],
      "metadata": {
        "id": "kvT_AWMSLHiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate a face embedding for each face in the dataset using facenet\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        "\n",
        "# get the face embedding for one face\n",
        "def get_embedding(model, face_pixels):\n",
        "\t# scale pixel values\n",
        "\tface_pixels = face_pixels.astype('float32')\n",
        "\t# standardize pixel values across channels (global)\n",
        "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
        "\tface_pixels = (face_pixels - mean) / std\n",
        "\t# transform face into one sample\n",
        "\tsamples = expand_dims(face_pixels, axis=0)\n",
        "\t# make prediction to get embedding\n",
        "\tyhat = model.predict(samples)\n",
        "\treturn yhat[0]\n",
        "\n",
        "# load the face dataset\n",
        "data = load('5-celebrity-faces-dataset.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "# load the facenet model\n",
        "modelem = load_model('./gdrive/My Drive/facenet/model/facenet_keras.h5')\n",
        "print('Loaded Model')\n",
        "# convert each face in the train set to an embedding\n",
        "newTrainX = list()\n",
        "for face_pixels in trainX:\n",
        "\tembedding = get_embedding(modelem, face_pixels)\n",
        "\tnewTrainX.append(embedding)\n",
        "newTrainX = asarray(newTrainX)\n",
        "print(newTrainX.shape)\n",
        "# convert each face in the test set to an embedding\n",
        "newTestX = list()\n",
        "for face_pixels in testX:\n",
        "\tembedding = get_embedding(modelem, face_pixels)\n",
        "\tnewTestX.append(embedding)\n",
        "newTestX = asarray(newTestX)\n",
        "print(newTestX.shape)\n",
        "# save arrays to one file in compressed format\n",
        "savez_compressed('5-celebrity-faces-embeddings.npz', newTrainX, trainy, newTestX, testy)"
      ],
      "metadata": {
        "id": "Z-5LHPOzLHl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "data = load('5-celebrity-faces-embeddings.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))"
      ],
      "metadata": {
        "id": "8KSUe0M5LY5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# develop a classifier for the 5 Celebrity Faces Dataset\n",
        "from random import choice\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot\n",
        "# load testing data faces\n",
        "data2 = load('5-celebrity-faces-dataset.npz')\n",
        "testX_faces = data2['arr_2']\n",
        "# load training dataface embeddings\n",
        "data = load('5-celebrity-faces-embeddings.npz')\n",
        "trainX, trainy, testXem, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testXem)\n",
        "#print(\"test embeddings\")\n",
        "#print(testXem)\n",
        "#print(testXem[0].shape)\n",
        "#print(\"test\")\n",
        "#print(testX)\n",
        "#print(testX.shape)\n",
        "#print(testX[0].shape)\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "# fit model\n",
        "modelsvc = SVC(kernel='linear', probability=True)\n",
        "modelsvc.fit(trainX, trainy)\n",
        "# test model on a random example from the test dataset\n",
        "selection = choice([i for i in range(testX.shape[0])])\n",
        "random_face_pixels = testX_faces[selection]\n",
        "#print(\"random_face_emb\",testX[selection])\n",
        "#print(testX[selection].shape)\n",
        "random_face_emb = testX[selection]\n",
        "random_face_class = testy[selection]\n",
        "random_face_name = out_encoder.inverse_transform([random_face_class])\n",
        "# prediction for the face\n",
        "samples = expand_dims(random_face_emb, axis=0)\n",
        "#print(\"samples:\",samples)\n",
        "#print(samples.shape)\n",
        "yhat_class = modelsvc.predict(samples)\n",
        "yhat_prob = modelsvc.predict_proba(samples)\n",
        "# get name\n",
        "class_index = yhat_class[0]\n",
        "class_probability = yhat_prob[0,class_index] * 100\n",
        "predict_names = out_encoder.inverse_transform(yhat_class)\n",
        "print('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\n",
        "print('Expected: %s' % random_face_name[0])\n",
        "# plot for fun\n",
        "pyplot.imshow(random_face_pixels)\n",
        "title = '%s (%.3f)' % (predict_names[0], class_probability)\n",
        "pyplot.title(title)\n",
        "pyplot.show()\n",
        "print(\"Predicted \",predict_names[0],\" with probability \",class_probability)"
      ],
      "metadata": {
        "id": "LqlyPSUHLY8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "\n",
        "from keras import applications as keras_applications\n",
        "\n",
        "!pip install keras_applications\n",
        "!pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface import utils\n"
      ],
      "metadata": {
        "id": "ytpTmhgHMH2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras import applications as keras_applications\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface import utils\n"
      ],
      "metadata": {
        "id": "FKhUug0WMH6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.system('tar -xf /kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/fer2013.tar.gz');\n",
        "#please replace the path mentioned below with the path of your fer2013 dataset.\n",
        "data = pd.read_csv('./gdrive/My Drive/facenet/fer2013/fer2013.csv')\n",
        "# ./gdrive/My Drive/facenet/images/5-celebrity-faces-dataset/train/\n",
        "data"
      ],
      "metadata": {
        "id": "ispn4fRVMP5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.Usage.unique()\n",
        "train_data = data[data.Usage=='Training']\n",
        "val_data = data[data.Usage=='PublicTest']\n",
        "test_data = data[data.Usage=='PrivateTest']\n",
        "train_data.shape, val_data.shape, test_data.shape"
      ],
      "metadata": {
        "id": "f_7cJmwWMP7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn\n",
        "import collections\n",
        "import imblearn\n",
        "from imblearn import under_sampling, over_sampling\n",
        "oversampler = imblearn.over_sampling.RandomOverSampler()"
      ],
      "metadata": {
        "id": "1yhEr0aYMP-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(train_data.emotion)"
      ],
      "metadata": {
        "id": "23EsyI6LMaGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "x_train, y_train = oversampler.fit_resample(train_data.pixels.values.reshape(-1,1),train_data.emotion.values)\n",
        "\n",
        "# x_train = train_data.pixels.values.reshape(-1,1)\n",
        "# y_train = train_data.emotion.values\n",
        "\n",
        "x_val = val_data.pixels.values.reshape(-1,1)\n",
        "y_val = val_data.emotion.values\n",
        "\n",
        "x_test = test_data.pixels.values.reshape(-1,1)\n",
        "y_test = test_data.emotion.values\n"
      ],
      "metadata": {
        "id": "ATsndooJMaIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(y_train)"
      ],
      "metadata": {
        "id": "EXeuROvtMaMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train = to_categorical(y_train,num_classes=7)\n",
        "y_val   = to_categorical(y_val  ,num_classes=7)\n",
        "y_test  = to_categorical(y_test ,num_classes=7)"
      ],
      "metadata": {
        "id": "ySbWzsTjMn2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy as copy\n",
        "\n",
        "def smooth_labels(y, smooth_factor):\n",
        "    '''Convert a matrix of one-hot row-vector labels into smoothed versions.\n",
        "\n",
        "    # Arguments\n",
        "        y: matrix of one-hot row-vector labels to be smoothed\n",
        "        smooth_factor: label smoothing factor (between 0 and 1)\n",
        "\n",
        "    # Returns\n",
        "        A matrix of smoothed labels.\n",
        "    '''\n",
        "    assert len(y.shape) == 2, 'input should be a batch of one-hot-encoded data'\n",
        "    y2 = copy(y)\n",
        "    if 0 <= smooth_factor <= 1:\n",
        "        # label smoothing ref: https://www.robots.ox.ac.uk/~vgg/rg/papers/reinception.pdf\n",
        "        y2 *= 1 - smooth_factor\n",
        "        y2 += smooth_factor / y.shape[1]\n",
        "    else:\n",
        "        raise Exception(\n",
        "            'Invalid label smoothing factor: ' + str(smooth_factor))\n",
        "    return y2"
      ],
      "metadata": {
        "id": "3rsgPrWHMn45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.utils import Sequence\n",
        "from keras.utils import to_categorical\n",
        "import cv2\n",
        "from math import floor\n",
        "\n",
        "class data_sequence(Sequence):\n",
        "    '''\n",
        "      yield sequence of data\n",
        "      features -- list of features\n",
        "      labels -- list of labels\n",
        "      target_channels {int} -- 1 (gray) or 3(RGB)\n",
        "    '''\n",
        "    def __init__(self, features, labels, batch_size=128, target_dim=(224,224),\n",
        "                 n_classes=7, shuffle=True, smooth=0.0):\n",
        "        'Initialization'\n",
        "        assert len(features)==len(labels), 'number of feature and labels not consistent'\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.target_dim = target_dim\n",
        "        self.target_channels = 3\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.smooth = smooth\n",
        "        self.sample_count = len(labels)\n",
        "        self.indexes = np.arange(self.sample_count)\n",
        "        self.on_epoch_end()\n",
        "#         self.verbose = verbose\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return floor(self.sample_count / self.batch_size)\n",
        "\n",
        "    def __gray2RGB__(self,x):\n",
        "      if len(x.shape)==2:\n",
        "        return np.stack((x,x,x),-1)\n",
        "      else:\n",
        "        assert len(x.shape)==3\n",
        "        if len(x[0,0,:]) == 1:\n",
        "          return np.stack((x[:,:,0],x[:,:,0],x[:,:,0]),-1)\n",
        "        else:\n",
        "          assert len(x[0,0,:])==self.target_channels\n",
        "      return x\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        X = np.empty((self.batch_size, *self.target_dim, self.target_channels))\n",
        "        Y = np.empty((self.batch_size, self.n_classes))\n",
        "        for i,ind in enumerate(indexes):\n",
        "          x = self.features[ind]\n",
        "          # resize image to the target size\n",
        "          x = cv2.resize(x,self.target_dim,interpolation=cv2.INTER_CUBIC)\n",
        "          x = self.__gray2RGB__(x)\n",
        "          X[i] = utils.preprocess_input(x, version=2) # or version=2 for VGGFace2 ResNet50\n",
        "          y = self.labels[ind]\n",
        "          if isinstance(y,int):\n",
        "            Y[i]=to_categorical(y,7)\n",
        "          else:\n",
        "            assert len(y)==self.n_classes\n",
        "            Y[i]=y\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "        if self.smooth > 0.0:\n",
        "          smooth_labels(Y, self.smooth)\n",
        "        return X,Y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "zgF9yjWoMn8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = list(x_train)\n",
        "x_val   = list(x_val)\n",
        "x_test  = list(x_test)\n",
        "\n",
        "for i,item in enumerate(x_train):\n",
        "    x_train[i] = np.fromstring(item[0],sep=' ').reshape(48,48,1)\n",
        "for i,item in enumerate(x_val):\n",
        "    x_val[i] = np.fromstring(item[0],sep=' ').reshape(48,48,1)\n",
        "for i,item in enumerate(x_test):\n",
        "    x_test[i] = np.fromstring(item[0],sep=' ').reshape(48,48,1)\n",
        "\n",
        "x_train = np.vstack(x_train).reshape(-1,48,48,1)\n",
        "x_val = np.vstack(x_val).reshape(-1,48,48,1)\n",
        "x_test = np.vstack(x_test).reshape(-1,48,48,1)"
      ],
      "metadata": {
        "id": "AHAmVa2AM0jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequence = data_sequence(x_train,y_train,batch_size=16,target_dim=(224,224),n_classes=7,shuffle=False)\n",
        "feature,lable = train_sequence.__getitem__(0)\n",
        "print(train_sequence)"
      ],
      "metadata": {
        "id": "Dzu9S0ftM0mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_dict = {0: 'Angry', 1:'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6:'Neutral'}\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(feature[0,:,:,:])\n",
        "plt.title(emotion_dict[np.argmax(lable[0])]);\n",
        "print(\"Predicted emotion\",emotion_dict[np.argmax(lable[0])])"
      ],
      "metadata": {
        "id": "2IXATu4AM0ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vggface = VGGFace(model='resnet50', include_top=False, input_shape = (224,224,3))\n",
        "vggface.trainable = False\n",
        "vggface.summary()"
      ],
      "metadata": {
        "id": "1V-dTes_NJnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "emotionalmodel = Sequential([vggface,\n",
        "                    Flatten(),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(2048, activation='relu'),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(1024, activation='relu'),\n",
        "                    Dense(7, activation='softmax', name = 'classifer')])\n",
        "emotionalmodel.summary()"
      ],
      "metadata": {
        "id": "Y7WUXaejNJpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequence = data_sequence(x_train,y_train,batch_size=64,target_dim=(224,224),n_classes=7,shuffle=True,smooth=0.07)\n",
        "val_sequence   = data_sequence(x_val,  y_val,  batch_size=64,target_dim=(224,224),n_classes=7,shuffle=True,smooth=0.0)\n",
        "test_sequence  = data_sequence(x_test, y_test, batch_size=64,target_dim=(224,224),n_classes=7,shuffle=True,smooth=0.0)\n"
      ],
      "metadata": {
        "id": "KqSP9ZE7NJsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionalmodel.compile(optimizer = keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist = emotionalmodel.fit_generator(generator = train_sequence,\n",
        "                            validation_data = val_sequence,\n",
        "                            epochs = 5)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist.history['accuracy'], marker='', color='blue', linewidth=3,label=\"train_accuracy\")\n",
        "plt.plot(hist.history['val_accuracy'], marker='', color='red', linewidth=3,label=\"val_accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist.history['loss'],marker='', color='blue', linewidth=3,label=\"train_loss\")\n",
        "plt.plot(hist.history['val_loss'],marker='', color='red', linewidth=3,label=\"val_loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "tqOlonOkNJvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionalmodel2 = Sequential([vggface,\n",
        "                    Flatten(),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(2048, activation='relu'),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(1024, activation='relu'),\n",
        "                    Dense(7, activation='softmax', name = 'classifer')])\n",
        "emotionalmodel2.compile(optimizer = keras.optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist2 = emotionalmodel2.fit_generator(generator = train_sequence,\n",
        "                            validation_data = val_sequence,\n",
        "                            epochs = 5)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist2.history['accuracy'], marker='', color='blue', linewidth=3,label=\"train_accuracy\")\n",
        "plt.plot(hist2.history['val_accuracy'], marker='', color='red', linewidth=3,label=\"val_accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist2.history['loss'],marker='', color='blue', linewidth=3,label=\"train_loss\")\n",
        "plt.plot(hist2.history['val_loss'],marker='', color='red', linewidth=3,label=\"val_loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "ZJ8xFxbxNZI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionalmodel3 = Sequential([vggface,\n",
        "                    Flatten(),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(2048, activation='relu'),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(1024, activation='relu'),\n",
        "                    Dense(7, activation='softmax', name = 'classifer')])\n",
        "emotionalmodel3.compile(optimizer = keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist3 = emotionalmodel3.fit_generator(generator = train_sequence,\n",
        "                            validation_data = val_sequence,\n",
        "                            epochs = 5)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist3.history['accuracy'], marker='', color='blue', linewidth=3,label=\"train_accuracy\")\n",
        "plt.plot(hist3.history['val_accuracy'], marker='', color='red', linewidth=3,label=\"val_accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist3.history['loss'],marker='', color='blue', linewidth=3,label=\"train_loss\")\n",
        "plt.plot(hist3.history['val_loss'],marker='', color='red', linewidth=3,label=\"val_loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "mw_m_e0RNZLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionalmodel4 = Sequential([vggface,\n",
        "                    Flatten(),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(2048, activation='relu'),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(1024, activation='relu'),\n",
        "                    Dense(7, activation='softmax', name = 'classifer')])\n",
        "emotionalmodel4.compile(optimizer = keras.optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist4 = emotionalmodel4.fit_generator(generator = train_sequence,\n",
        "                            validation_data = val_sequence,\n",
        "                            epochs = 10)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist4.history['accuracy'], marker='', color='blue', linewidth=3,label=\"train_accuracy\")\n",
        "plt.plot(hist4.history['val_accuracy'], marker='', color='red', linewidth=3,label=\"val_accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist4.history['loss'],marker='', color='blue', linewidth=3,label=\"train_loss\")\n",
        "plt.plot(hist4.history['val_loss'],marker='', color='red', linewidth=3,label=\"val_loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "lupYhiTdNZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionalmodel5 = Sequential([vggface,\n",
        "                    Flatten(),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(2048, activation='relu'),\n",
        "                    Dropout(0.25),\n",
        "                    Dense(1024, activation='relu'),\n",
        "                    Dense(7, activation='softmax', name = 'classifer')])\n",
        "emotionalmodel5.compile(optimizer = keras.optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "hist5 = emotionalmodel5.fit_generator(generator = train_sequence,\n",
        "                            validation_data = val_sequence,\n",
        "                            epochs = 20)\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist5.history['accuracy'], marker='', color='blue', linewidth=3,label=\"train_accuracy\")\n",
        "plt.plot(hist5.history['val_accuracy'], marker='', color='red', linewidth=3,label=\"val_accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist5.history['loss'],marker='', color='blue', linewidth=3,label=\"train_loss\")\n",
        "plt.plot(hist5.history['val_loss'],marker='', color='red', linewidth=3,label=\"val_loss\")\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "9yVPJW7dNods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,3))\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(hist5.history['val_accuracy'], marker='', color='red', linewidth=3,label=\"20epoch\")\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(hist4.history['val_accuracy'], marker='', color='blue', linewidth=3,label=\"10epochs\")\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(hist2.history['val_accuracy'], marker='', color='green', linewidth=3,label=\"5epochs\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "shpbZkaSNogO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotionalmodel4.evaluate(test_sequence)"
      ],
      "metadata": {
        "id": "Ta55dT0CNojq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature2,lable2 = test_sequence.__getitem__(0)\n",
        "emotion_dict = {0: 'Angry', 1:'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6:'Neutral'}\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(feature2[4,:,:,:])\n",
        "print(\"Expected emotion:\",emotion_dict[np.argmax(lable2[4])])\n",
        "res=emotionalmodel4.predict(feature2)\n",
        "print(\"Predicted Emotion:\",emotion_dict[np.argmax(res[4])],\"with probabiltity:\",res[4][np.argmax(res[4])])"
      ],
      "metadata": {
        "id": "D5u4pyazN0LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for x,y in test_sequence:\n",
        "    y_pred = y_pred + list(emotionalmodel4.predict_classes(x))\n",
        "    y_true = y_true + list(np.argmax(y,axis=1))"
      ],
      "metadata": {
        "id": "yom1WjiwN0No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()\n",
        "confusion_matrix = np.around(confusion_matrix.astype('float') / confusion_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "confusion_matrix = pd.DataFrame(confusion_matrix,\n",
        "                                index = emotion_dict.values(),\n",
        "                                columns = emotion_dict.values())"
      ],
      "metadata": {
        "id": "P0ME_ceoN8yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UfnmDK6iN81e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(image):\n",
        "  #face recognition\n",
        "  image2 = cv2.imread(image)\n",
        "  image2=cv2.cvtColor(image2,cv2.COLOR_BGR2RGB)\n",
        "#Show the image with matplotlib\n",
        "  plt.imshow(image2)\n",
        "  plt.show()\n",
        "  facearray=extract_face(image,(160, 160))\n",
        "  embedding2 = get_embedding(modelem, facearray)\n",
        "  li=[]\n",
        "  li.append(embedding2)\n",
        "  in_encoder = Normalizer(norm='l2')\n",
        "  random_face_emb2 = in_encoder.transform(li)\n",
        "  samples2 = expand_dims(random_face_emb2[0], axis=0)\n",
        "  yhat_class2 = modelsvc.predict(samples2)\n",
        "  yhat_prob2 = modelsvc.predict_proba(samples)\n",
        "  class_index2 = yhat_class2[0]\n",
        "  predict_names2 = out_encoder.inverse_transform(yhat_class2)\n",
        "\n",
        "  #emotion detection\n",
        "  facearrayem=extract_face(image,(224, 224))\n",
        "  facearrayem = facearrayem.astype('float32')\n",
        "  X = utils.preprocess_input(facearrayem, version=2) # or version=2 for VGGFace2 ResNet50\n",
        "  X = np.array(X)\n",
        "  ls=np.array([X])\n",
        "  res2=emotionalmodel4.predict(ls)\n",
        "  pyplot.imshow(facearray)\n",
        "  pyplot.show()\n",
        "  print(predict_names2[0],\"is feeling\",emotion_dict[np.argmax(res2[0])],\"today\")\n",
        "  print(\"Face Recognition probability:\",yhat_prob2[0][np.argmax(yhat_prob2)],\"Emotion Detection probability:\",res2[0][np.argmax(res2[0])])\n"
      ],
      "metadata": {
        "id": "1wt2N4CmN843"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enter the path of the image that you would want to test for face recognition and  emotion detection\n",
        "pred('/content/gdrive/MyDrive/facenet/images/5-celebrity-faces-dataset/aishwarya.jpeg')\n"
      ],
      "metadata": {
        "id": "ZmG5m5VBN0Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred('/content/gdrive/MyDrive/facenet/images/5-celebrity-faces-dataset/mindyangry.jpg')"
      ],
      "metadata": {
        "id": "QRON0YstONA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred('/content/gdrive/MyDrive/facenet/images/5-celebrity-faces-dataset/jerry2.jpg')"
      ],
      "metadata": {
        "id": "xPpUQs_CONEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred('/content/gdrive/MyDrive/facenet/images/5-celebrity-faces-dataset/madonnacry.jpg')"
      ],
      "metadata": {
        "id": "WcCoMx1VOaYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eydZ7gWOabx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}